\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote.
% If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{caption}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{multirow}
\usepackage{color, soul}
\usepackage{blindtext}

\pagestyle{fancy}
\fancyfoot{}
\cfoot{\thepage}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}


\begin{document}

%% \title{
%%    Facial Expression Recognition\\
%%    using a\\
%%    Spiking Neural Network
%% {\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
%% should not be used}
%% \thanks{Identify applicable funding agency here. If none, delete this.}
%% }
%%
%% \author{
%%     \IEEEauthorblockN{Anna Lee}
%%     \IEEEauthorblockA{\textit{College of Engineering and Computing} \\
%%     \textit{University of South Carolina}\\
%%     Columbia, SC, USA \\
%%     aml8@email.sc.edu}
%%     \and
%%     \IEEEauthorblockN{Heath Smith}
%%     \IEEEauthorblockA{\textit{College of Engineering and Computing} \\
%%     \textit{University of South Carolina}\\
%%     Columbia, SC, USA \\
%%     smithmh6@email.sc.edu}
%% }
%%
%% \maketitle
%%
%% %% Abstract
%% \input{sections/abstract.tex}
%%
%% \begin{IEEEkeywords}
%%      Artificial Intelligence, Machine Learning, Deep Learning,
%%      Spiking Neural Network, Convolutional Neural Network, Python,
%%      Facial Expression Recognition
%% \end{IEEEkeywords}

Much research has been completed in regard to accomplishing facial expression recognition with different
neural network models and facial expression datasets. There are a number of different datasets that are
typically used for facial expression recognition research, including the \textit{Japanese Female Facial Expression}(JAFFE)
dataset, the \textit{Extended Cohn-Kanade}(CK+) dataset, and the \textit{FER-2013} dataset. The JAFFE dataset was created
to test models used for facial expression recognition against human perception\cite{jaffe}. JAFFE contains $213$ images of \textit{seven}
basic facial expressions(happiness, surprise, sadness, anger, disgust, fear, neutral) posed by \textit{ten} different
Japanese female models. The CK+ dataset was created by Lucey et al. \cite{ck_plus} and consists of $593$ video sequences,
each of which contains a facial shift from a neutral expression to either happiness, sadness, surprise, anger, contempt,
disgust, or fear. Additionally, it was created from $123$ different subjects consisting of a variety of age ranges,
genders, and heritages. As described in \cite{fer2013}, FER-2013 was created by Carrier and Courville as part of a contest
to create a new system for facial expression recognition that included the creation of an entirely new dataset. FER-2013
consists of $35,887$ grayscale images in total, $28,709$ of which are intended for training while the other $7,178$ are
intended to be used for testing.

The present difficulty in implementing SNNs via software makes it more desireable to train a
CNN then use the weights and activations to convert the CNN to a SNN. After conversion, SNNs are
more readily deployable in real world systems due to the ability to reduce computational overhead.
In terms of neural network models, Mollahosseini et al. \cite{mollahosseini} created a Deep Neural
Network(DNN) architecture for facial expression recognition that consisted of \textit{two} convolutional
layers and \textit{two} max-pooling layers followed by \textit{four} Inception\cite{inception} layers,
a component inspired by GoogLeNet. Fathallah et al. \cite{fathallah} proposed a CNN architecture, based
on the \textit{Visual Geometry Group}(VGG) model\cite{vggnet}, that consisted of \textit{four} convolutional
layers and \textit{three} max-pooling layers that were followed by a fully-connected layer and a softmax
output layer. Furthermore, Kandeel et al. \cite{kandeel} proposed an architecture for facial expression
recognition consisting of \textit{three} convolutional layers and \textit{three} max-pooling layers
followed by \textit{two} fully-connected layers and a softmax output layer for classification of the
expression classes. In an attempt to develop new techniques for facial expression recognition, Mansouri-Benssassi
and Ye \cite{mansouri} proposed a bio-inspired spiking neural network model, which had not been explored
previously. Their model consisted of an input layer with $10,000$ neurons, a convolutional layer with
$50$ features, a stride size of $15$, and a convolution size of $15$, an excitatory layer, and a single inhibitory
layer. They also included an image preprocessinglayer where a \textit{Laplacian of Gaussian} filter is applied to
the image for edge detection, and then spike trains are created using a Poisson distribution.

\begin{thebibliography}{00}

    % SNN conversion
    % \bibitem{petro} B. Petro, N. Kasabov and R. M. Kiss, ``Selection and Optimization of Temporal Spike Encoding Methods for Spiking Neural Networks``, in IEEE Transactions on Neural Networks and Learning Systems, vol. 31, no. 2, pp. 358-370, Feb. 2020, doi: 10.1109/TNNLS.2019.2906158.
    % \bibitem{chand} P. Chandarana, J. Ou and R. Zand, ``An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks,`` 2021 12th International Green and Sustainable Computing Conference (IGSC), 2021, pp. 1-8, doi: 10.1109/IGSC54211.2021.9651610.
    % \bibitem{snnconv} B. Rueckauer, I. Lungu, Y. Hu, M. Pfeiffer, S. Liu, ``Conversion of Continuous-Valued Deep Networks to Efficient Event-Driven Networks for Image Classification``, Front. Neurosci., 07 December 2017
    % \bibitem{temporalcoding} B. Rueckauer and S. -C. Liu, ``Conversion of analog to spiking neural networks using sparse temporal coding``, 2018 IEEE International Symposium on Circuits and Systems (ISCAS), 2018, pp. 1-5, doi: 10.1109/ISCAS.2018.8351295.
    % \bibitem{vgg_architecture} A. Kaushik, OpenGenus IQ -- Computing Expertise \& Legacy. 2022. Understanding the VGG19 Architecture. [online] Available at: \url{https://iq.opengenus.org/vgg19-architecture/} Accessed 13 March 2022.

    % datasets
    \bibitem{fer2013}  I. J. Goodfellow, D. Erhan, P. L. Carrier, A. Courville, M. Mirza, B. Hamner, W. Cukierski, Y. Tang, D. Thaler, D.-H. Lee, Y. Zhou, C. Ramaiah, F. Feng, R. Li, X. Wang, D. Athanasakis, J. Shawe-Taylor, M. Milakov, J. Park, R. Ionescu, M. Popescu, C. Grozea, J. Bergstra, J. Xie, L. Romaszko, B. Xu, Z. Chuang, and Y. Bengio. Challenges in representation learning: A report on three machine learning contests. Neural Networks, 64:59--63, 2015. Special Issue on "Deep Learning of Representations"
    \bibitem{jaffe}	M. Lyons, “'Excavating AI' Re-excavated: Debunking a Fallacious Account of the JAFFE Dataset,” arXiv:2107.13998v1 [cs.CY], Jul. 2021.
    \bibitem{ck_plus}	P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar and I. Matthews, "The Extended Cohn-Kanade Dataset (CK+): A complete dataset for action unit and emotion-specified expression," 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops, 2010, pp. 94-101, doi: 10.1109/CVPRW.2010.5543262.

    % CNNs
    \bibitem{vggnet} K. Simonyan, A. Zisserman, ``Very Deep Convolutional Networks for Large-Scale Image Recognition``, arXiv:1409.1556v6 [cs.CV] 10 Apr 2015.
    \bibitem{inception} C. Szegedy et al., ``Going deeper with convolutions,`` 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 1-9, doi: 10.1109/CVPR.2015.7298594.
    \bibitem{mollahosseini}	A. Mollahosseini, D. Chan and M. H. Mahoor, "Going deeper in facial expression recognition using deep neural networks," 2016 IEEE Winter Conference on Applications of Computer Vision (WACV), 2016, pp. 1-10, doi: 10.1109/WACV.2016.7477450.
    \bibitem{fathallah}	A. Fathallah, L. Abdi and A. Douik, "Facial Expression Recognition via Deep Learning," 2017 IEEE/ACS 14th International Conference on Computer Systems and Applications (AICCSA), 2017, pp. 745-750, doi: 10.1109/AICCSA.2017.124.
    \bibitem{kandeel}	A. Kandeel, M. Rahmanian, F. Zulkernine, H. M. Abbas and H. Hassanein, "Facial Expression Recognition Using a Simplified Convolutional Neural Network Model," 2020 International Conference on Communications, Signal Processing, and their Applications (ICCSPA), 2021, pp. 1-6, doi: 10.1109/ICCSPA49915.2021.9385739.
    \bibitem{mansouri}	Mansouri-Benssassi, E., Ye, J. (2018). Bio-Inspired Spiking Neural Networks for Facial Expression Recognition: Generalisation Investigation. In: Fagan, D., Martín-Vide, C., O'Neill, M., Vega-Rodríguez, M.A. (eds) Theory and Practice of Natural Computing. TPNC 2018. Lecture Notes in Computer Science(), vol 11324. Springer, Cham. \url{https://doi.org/10.1007/978-3-030-04070-3_33}

\end{thebibliography}
\vspace{12pt}

\end{document}