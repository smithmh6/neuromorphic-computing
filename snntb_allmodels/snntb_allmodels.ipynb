{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import configparser\n",
    "import h5py\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pathlib\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "import utils\n",
    "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n",
    "import snntoolbox.bin.run as snn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15339 examples in 7 classes.\n",
      "Shape= (48, 48)\n",
      "Class Names= ['Surprise' 'Fear' 'Disgust' 'Happiness' 'Sadness' 'Anger' 'Neutral']\n",
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-06 23:17:56.421292: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-03-06 23:17:56.421400: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Dataset Info:\n",
      "---> size: 24.73805 mb\n",
      "\n",
      "---> dimensions: (48, 48)\n",
      "---> samples: 10737\n",
      "---> batches: 335\n",
      "\n",
      "Expanding image dimensions...\n",
      "---> elapsed time: 17.03762 ms\n",
      "\n",
      "Dataset size after reshaping:\n",
      "---> size: 24.73805 mb\n",
      "\n",
      "Creating dataset object..\n",
      "\n",
      "Batch shapes: \n",
      "(335, 32, 48, 48, 1) (335, 32, 7)\n",
      "---> elapsed time: 11507.73179 ms\n",
      "\n",
      "Setting batch size to 32\n",
      "---> elapsed time: 1.86892 ms\n",
      "\n",
      "Configuring prefetching...\n",
      "---> elapsed time: 0.73613 ms\n",
      "\n",
      "Scaling images...\n",
      "---> elapsed time: 85.25567 ms\n",
      "\n",
      "Configuring caching...\n",
      "---> elapsed time: 0.18029 ms\n",
      "\n",
      "Total time ===> 11.612866624999999 s\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Dataset Info:\n",
      "---> size: 5.3015 mb\n",
      "\n",
      "---> dimensions: (48, 48)\n",
      "---> samples: 2301\n",
      "---> batches: 71\n",
      "\n",
      "Expanding image dimensions...\n",
      "---> elapsed time: 3.60783 ms\n",
      "\n",
      "Dataset size after reshaping:\n",
      "---> size: 5.3015 mb\n",
      "\n",
      "Creating dataset object..\n",
      "\n",
      "Batch shapes: \n",
      "(71, 32, 48, 48, 1) (71, 32, 7)\n",
      "---> elapsed time: 2479.92829 ms\n",
      "\n",
      "Setting batch size to 32\n",
      "---> elapsed time: 0.29329 ms\n",
      "\n",
      "Configuring prefetching...\n",
      "---> elapsed time: 0.12175 ms\n",
      "\n",
      "Scaling images...\n",
      "---> elapsed time: 3.3955 ms\n",
      "\n",
      "Configuring caching...\n",
      "---> elapsed time: 0.12733 ms\n",
      "\n",
      "Total time ===> 2.487506374999999 s\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Dataset Info:\n",
      "---> size: 3.53434 mb\n",
      "\n",
      "---> dimensions: (48, 48)\n",
      "---> samples: 1534\n",
      "---> batches: 47\n",
      "\n",
      "Expanding image dimensions...\n",
      "---> elapsed time: 2.38854 ms\n",
      "\n",
      "Dataset size after reshaping:\n",
      "---> size: 3.53434 mb\n",
      "\n",
      "Creating dataset object..\n",
      "\n",
      "Batch shapes: \n",
      "(47, 32, 48, 48, 1) (47, 32, 7)\n",
      "---> elapsed time: 1651.7905 ms\n",
      "\n",
      "Setting batch size to 32\n",
      "---> elapsed time: 0.29971 ms\n",
      "\n",
      "Configuring prefetching...\n",
      "---> elapsed time: 0.12379 ms\n",
      "\n",
      "Scaling images...\n",
      "---> elapsed time: 3.55812 ms\n",
      "\n",
      "Configuring caching...\n",
      "---> elapsed time: 0.12458 ms\n",
      "\n",
      "Total time ===> 1.6583211660000003 s\n",
      "--------------------------------------------------------------------------------\n",
      "Using 10752 samples for training.\n",
      "Using 2304 samples for validation.\n",
      "Using 1536 samples for testing.\n"
     ]
    }
   ],
   "source": [
    "# create datasets\n",
    "data_path = pathlib.Path(r'../datasets/RAFDB.npz')\n",
    "with np.load(data_path) as data:\n",
    "    images = data['images']\n",
    "    labels = data['labels']\n",
    "    classes = data['classes']\n",
    "print(f\"Found {images.shape[0]} examples in {labels.shape[1]} classes.\")\n",
    "print(f\"Shape= {(images.shape[1], images.shape[2])}\")\n",
    "print(f\"Class Names= {classes}\")\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "# constants\n",
    "CLASS_LABELS = classes\n",
    "NUM_CLASSES = len(classes)\n",
    "BATCH_SIZE = 32\n",
    "IMAGE_DIMS = (48, 48)\n",
    "SEED = 123\n",
    "_TRAIN = .7\n",
    "_VAL = .15\n",
    "_TEST = .1\n",
    "\n",
    "# create a dataset object and shuffle\n",
    "dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "dataset = dataset.shuffle(len(images) * BATCH_SIZE, seed=SEED)\n",
    "shuffle_images = []\n",
    "shuffle_labels = []\n",
    "for im, l in dataset.as_numpy_iterator():\n",
    "    shuffle_images.append(im)\n",
    "    shuffle_labels.append(l)\n",
    "shuffle_images = np.asarray(shuffle_images)\n",
    "shuffle_labels = np.asarray(shuffle_labels)\n",
    "\n",
    "# create the slicing indices\n",
    "TRAIN = round(len(dataset) * _TRAIN)\n",
    "VAL = TRAIN + round(len(dataset) * _VAL)\n",
    "TEST = VAL + round(len(dataset) * _TEST)\n",
    "\n",
    "train_ds = utils.data_pipeline(\n",
    "    shuffle_images[0:TRAIN], shuffle_labels[0:TRAIN], IMAGE_DIMS,\n",
    "    edges=False, batch_size=BATCH_SIZE, flip=False, haar=False)\n",
    "val_ds = utils.data_pipeline(\n",
    "    shuffle_images[TRAIN:VAL], shuffle_labels[TRAIN:VAL], IMAGE_DIMS,\n",
    "    edges=False, batch_size=BATCH_SIZE, flip=False, haar=False)\n",
    "test_ds = utils.data_pipeline(\n",
    "    shuffle_images[VAL:TEST], shuffle_labels[VAL:TEST], IMAGE_DIMS,\n",
    "    edges=False, batch_size=BATCH_SIZE, flip=False, haar=False)\n",
    "\n",
    "\n",
    "print(f\"Using {len(train_ds)*BATCH_SIZE} samples for training.\")\n",
    "print(f\"Using {len(val_ds)*BATCH_SIZE} samples for validation.\")\n",
    "print(f\"Using {len(test_ds)*BATCH_SIZE} samples for testing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-06 23:18:12.996876: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "# store the data for SNN conversion/simulation\n",
    "x_test = []\n",
    "y_test = []\n",
    "x_norm = []\n",
    "\n",
    "# create the dataset for SNN simulation\n",
    "for im, l in test_ds.as_numpy_iterator():\n",
    "    for i in range(np.shape(im)[0]):\n",
    "        #print(im[i, :, :, :].shape)\n",
    "        x_test.append(im[i, :, :, :])\n",
    "        y_test.append(l[i, :])\n",
    "\n",
    "# create a dataset for normalization\n",
    "for im, l in list(train_ds.as_numpy_iterator()):\n",
    "    for i in range(np.shape(im)[0]):\n",
    "        #print(im[i, :, :, :].shape)\n",
    "        x_norm.append(im[i, :, :, :])\n",
    "\n",
    "x_test = np.asarray(x_test)\n",
    "y_test = np.asarray(y_test)\n",
    "x_norm = np.asarray(x_norm)\n",
    "### print(x_test.shape)\n",
    "### print(y_test.shape)\n",
    "\n",
    "### x_test = np.load(os.path.join('./data', 'x_test.npy'))\n",
    "### y_test = np.load(os.path.join('./data', 'y_test.npy'))\n",
    "### \n",
    "### print(f\"X_Data shape ---> {x_test.shape}\")\n",
    "### print(f\"Y_Data shape ---> {y_test.shape}\")\n",
    "### \n",
    "### np.savez_compressed(r'./data/x_test', x_test)\n",
    "### np.savez_compressed(r'./data/y_test', y_test)\n",
    "### np.savez_compressed(r'./data/x_norm', x_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential_1 (Sequential)   (None, 48, 48, 12)        1476      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 24, 24, 12)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " sequential_2 (Sequential)   (None, 24, 24, 24)        7920      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 12, 12, 24)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " sequential_3 (Sequential)   (None, 12, 12, 44)        27192     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 6, 6, 44)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1584)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 85)                134725    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 85)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               8600      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 7)                 707       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 180,620\n",
      "Trainable params: 180,460\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_name = \"VGG_b3k112k224k344k460u185u2100p1_0.16p2_0.21\"\n",
    "test_model = tf.keras.models.load_model(f'./models/{model_name}.h5')\n",
    "test_model.summary()\n",
    "\n",
    "train_new = True\n",
    "\n",
    "if train_new == False:\n",
    "    model = test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.layers.reshaping.flatten.Flatten object at 0x152673250>\n",
      "<keras.layers.core.dense.Dense object at 0x15267f1f0>\n",
      "<keras.layers.regularization.dropout.Dropout object at 0x15267f970>\n",
      "<keras.layers.core.dense.Dense object at 0x15268d880>\n",
      "<keras.layers.regularization.dropout.Dropout object at 0x15268d790>\n",
      "<keras.layers.core.dense.Dense object at 0x152697cd0>\n"
     ]
    }
   ],
   "source": [
    "## tf.keras.utils.plot_model(\n",
    "##     model,\n",
    "##     to_file='./out/test_architecture.png',\n",
    "##     show_shapes=True,\n",
    "##     show_dtype=False,\n",
    "##     show_layer_names=True,\n",
    "##     rankdir='TB',\n",
    "##     expand_nested=False,\n",
    "##     dpi=150,\n",
    "##     layer_range=None,\n",
    "##     show_layer_activations=True\n",
    "## )\n",
    "\n",
    "\n",
    "for layer in test_model.layers[6:]:\n",
    "    print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 48, 48, 12)        120       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 48, 48, 12)        1308      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 24, 24, 12)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 24, 24, 24)        2616      \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 24, 24, 24)        5208      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 12, 12, 24)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 12, 12, 44)        9548      \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 12, 12, 44)        17468     \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 12, 12, 44)       176       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 6, 6, 44)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1584)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 85)                134725    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 85)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               8600      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 7)                 707       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 180,476\n",
      "Trainable params: 180,388\n",
      "Non-trainable params: 88\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "if train_new:\n",
    "    # build model\n",
    "    model = tf.keras.models.Sequential([\n",
    "\n",
    "        tf.keras.layers.Input(shape=(48, 48, 1)),\n",
    "\n",
    "        tf.keras.layers.Conv2D(filters=12, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"),\n",
    "        tf.keras.layers.Conv2D(filters=12, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"),\n",
    "        #tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
    "\n",
    "\n",
    "        tf.keras.layers.Conv2D(filters=24, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"),\n",
    "        tf.keras.layers.Conv2D(filters=24, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"),\n",
    "        #tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
    "\n",
    "        tf.keras.layers.Conv2D(filters=44, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"),\n",
    "        tf.keras.layers.Conv2D(filters=44, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
    "\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(85, activation = 'relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(100, activation = 'relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(7, activation = 'softmax'),\n",
    "\n",
    "    ])\n",
    "    ## for layer in test_model.layers[6:]:\n",
    "    ##     print(f\"Adding layer {layer.name}\")\n",
    "    ##     model.add(layer)\n",
    "\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_new == False:\n",
    "    # copy Sequential_1\n",
    "    for i in range(0, 3):\n",
    "        print(f\"Copying weights from {test_model.layers[0].layers[i].name} ---> {model.layers[i].name}\")\n",
    "        model.layers[i].set_weights(test_model.layers[0].layers[i].get_weights())\n",
    "\n",
    "    # copy max pooling 1\n",
    "    print(f\"Copying weights from {test_model.layers[1].name} ---> {model.layers[3].name}\")\n",
    "    model.layers[3].set_weights(test_model.layers[1].get_weights())\n",
    "\n",
    "    # copy Sequential_2\n",
    "    for i in range(4, 7):\n",
    "        print(f\"Copying weights from {test_model.layers[2].layers[i - 4].name} ---> {model.layers[i].name}\")\n",
    "        model.layers[i].set_weights(test_model.layers[2].layers[i - 4].get_weights())\n",
    "\n",
    "    # copy max pooling 2\n",
    "    print(f\"Copying weights from {test_model.layers[3].name} ---> {model.layers[7].name}\")\n",
    "    model.layers[7].set_weights(test_model.layers[3].get_weights())\n",
    "\n",
    "    # copy Sequential_3\n",
    "    for i in range(8, 11):\n",
    "        print(f\"Copying weights from {test_model.layers[4].layers[i - 8].name} ---> {model.layers[i].name}\")\n",
    "        model.layers[i].set_weights(test_model.layers[4].layers[i - 8].get_weights())\n",
    "\n",
    "    # copy max pooling 3\n",
    "    print(f\"Copying weights from {test_model.layers[5].name} ---> {model.layers[7].name}\")\n",
    "    model.layers[11].set_weights(test_model.layers[5].get_weights())\n",
    "\n",
    "\n",
    "    # copy remaining layers\n",
    "    print(\"Remaing layers -->\")\n",
    "    for i in range(12, len(model.layers)):\n",
    "        print(f\"Copying weights from {test_model.layers[i - 6].name} ---> {model.layers[i].name}\")\n",
    "        model.layers[i].set_weights(test_model.layers[i - 6].get_weights())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configuring early stopping parameters.\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-06 23:18:14.064622: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336/336 [==============================] - ETA: 0s - loss: 1.6345 - accuracy: 0.3904"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-06 23:18:20.655481: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: saving model to ./out/VGG_b3k112k224k344k460u185u2100p1_0.16p2_0.21.h5\n",
      "336/336 [==============================] - 8s 21ms/step - loss: 1.6345 - accuracy: 0.3904 - val_loss: 1.7822 - val_accuracy: 0.4363\n",
      "Epoch 2/25\n",
      "335/336 [============================>.] - ETA: 0s - loss: 1.3679 - accuracy: 0.5007\n",
      "Epoch 2: saving model to ./out/VGG_b3k112k224k344k460u185u2100p1_0.16p2_0.21.h5\n",
      "336/336 [==============================] - 8s 22ms/step - loss: 1.3672 - accuracy: 0.5009 - val_loss: 1.2646 - val_accuracy: 0.5602\n",
      "Epoch 3/25\n",
      "334/336 [============================>.] - ETA: 0s - loss: 1.2001 - accuracy: 0.5672\n",
      "Epoch 3: saving model to ./out/VGG_b3k112k224k344k460u185u2100p1_0.16p2_0.21.h5\n",
      "336/336 [==============================] - 7s 22ms/step - loss: 1.1995 - accuracy: 0.5676 - val_loss: 1.1068 - val_accuracy: 0.6093\n",
      "Epoch 4/25\n",
      "335/336 [============================>.] - ETA: 0s - loss: 1.1049 - accuracy: 0.6043\n",
      "Epoch 4: saving model to ./out/VGG_b3k112k224k344k460u185u2100p1_0.16p2_0.21.h5\n",
      "336/336 [==============================] - 7s 21ms/step - loss: 1.1041 - accuracy: 0.6045 - val_loss: 1.0883 - val_accuracy: 0.5997\n",
      "Epoch 5/25\n",
      "336/336 [==============================] - ETA: 0s - loss: 1.0326 - accuracy: 0.6326\n",
      "Epoch 5: saving model to ./out/VGG_b3k112k224k344k460u185u2100p1_0.16p2_0.21.h5\n",
      "336/336 [==============================] - 7s 21ms/step - loss: 1.0326 - accuracy: 0.6326 - val_loss: 1.1388 - val_accuracy: 0.5950\n",
      "Epoch 6/25\n",
      "336/336 [==============================] - ETA: 0s - loss: 0.9762 - accuracy: 0.6536\n",
      "Epoch 6: saving model to ./out/VGG_b3k112k224k344k460u185u2100p1_0.16p2_0.21.h5\n",
      "336/336 [==============================] - 7s 21ms/step - loss: 0.9762 - accuracy: 0.6536 - val_loss: 1.2474 - val_accuracy: 0.5385\n",
      "Epoch 7/25\n",
      "335/336 [============================>.] - ETA: 0s - loss: 0.9227 - accuracy: 0.6738\n",
      "Epoch 7: saving model to ./out/VGG_b3k112k224k344k460u185u2100p1_0.16p2_0.21.h5\n",
      "336/336 [==============================] - 7s 21ms/step - loss: 0.9220 - accuracy: 0.6740 - val_loss: 1.1418 - val_accuracy: 0.5750\n",
      "Epoch 8/25\n",
      "336/336 [==============================] - ETA: 0s - loss: 0.8743 - accuracy: 0.6892\n",
      "Epoch 8: saving model to ./out/VGG_b3k112k224k344k460u185u2100p1_0.16p2_0.21.h5\n",
      "336/336 [==============================] - 7s 21ms/step - loss: 0.8743 - accuracy: 0.6892 - val_loss: 1.1633 - val_accuracy: 0.5663\n",
      "Epoch 9/25\n",
      "336/336 [==============================] - ETA: 0s - loss: 0.8278 - accuracy: 0.7072\n",
      "Epoch 9: saving model to ./out/VGG_b3k112k224k344k460u185u2100p1_0.16p2_0.21.h5\n",
      "336/336 [==============================] - 7s 21ms/step - loss: 0.8278 - accuracy: 0.7072 - val_loss: 1.0582 - val_accuracy: 0.6110\n",
      "Epoch 10/25\n",
      "336/336 [==============================] - ETA: 0s - loss: 0.7816 - accuracy: 0.7269\n",
      "Epoch 10: saving model to ./out/VGG_b3k112k224k344k460u185u2100p1_0.16p2_0.21.h5\n",
      "336/336 [==============================] - 7s 21ms/step - loss: 0.7816 - accuracy: 0.7269 - val_loss: 1.2585 - val_accuracy: 0.5354\n",
      "Epoch 11/25\n",
      "335/336 [============================>.] - ETA: 0s - loss: 0.7397 - accuracy: 0.7418\n",
      "Epoch 11: saving model to ./out/VGG_b3k112k224k344k460u185u2100p1_0.16p2_0.21.h5\n",
      "336/336 [==============================] - 7s 21ms/step - loss: 0.7390 - accuracy: 0.7420 - val_loss: 1.3197 - val_accuracy: 0.5163\n",
      "Epoch 12/25\n",
      "336/336 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.7630\n",
      "Epoch 12: saving model to ./out/VGG_b3k112k224k344k460u185u2100p1_0.16p2_0.21.h5\n",
      "336/336 [==============================] - 7s 21ms/step - loss: 0.6936 - accuracy: 0.7630 - val_loss: 1.2269 - val_accuracy: 0.5550\n",
      "Epoch 13/25\n",
      "335/336 [============================>.] - ETA: 0s - loss: 0.6529 - accuracy: 0.7701\n",
      "Epoch 13: saving model to ./out/VGG_b3k112k224k344k460u185u2100p1_0.16p2_0.21.h5\n",
      "336/336 [==============================] - 7s 21ms/step - loss: 0.6523 - accuracy: 0.7703 - val_loss: 1.2732 - val_accuracy: 0.5376\n",
      "Epoch 14/25\n",
      "335/336 [============================>.] - ETA: 0s - loss: 0.6079 - accuracy: 0.7902\n",
      "Epoch 14: saving model to ./out/VGG_b3k112k224k344k460u185u2100p1_0.16p2_0.21.h5\n",
      "336/336 [==============================] - 7s 21ms/step - loss: 0.6073 - accuracy: 0.7904 - val_loss: 1.2227 - val_accuracy: 0.5619\n",
      "Epoch 15/25\n",
      "335/336 [============================>.] - ETA: 0s - loss: 0.5680 - accuracy: 0.8051\n",
      "Epoch 15: saving model to ./out/VGG_b3k112k224k344k460u185u2100p1_0.16p2_0.21.h5\n",
      "336/336 [==============================] - 7s 21ms/step - loss: 0.5675 - accuracy: 0.8053 - val_loss: 1.2430 - val_accuracy: 0.5511\n",
      "Epoch 16/25\n",
      "336/336 [==============================] - ETA: 0s - loss: 0.5271 - accuracy: 0.8221\n",
      "Epoch 16: saving model to ./out/VGG_b3k112k224k344k460u185u2100p1_0.16p2_0.21.h5\n",
      "336/336 [==============================] - 7s 21ms/step - loss: 0.5271 - accuracy: 0.8221 - val_loss: 1.1382 - val_accuracy: 0.6019\n",
      "Epoch 17/25\n",
      "336/336 [==============================] - ETA: 0s - loss: 0.4833 - accuracy: 0.8423\n",
      "Epoch 17: saving model to ./out/VGG_b3k112k224k344k460u185u2100p1_0.16p2_0.21.h5\n",
      "336/336 [==============================] - 7s 21ms/step - loss: 0.4833 - accuracy: 0.8423 - val_loss: 1.1052 - val_accuracy: 0.6219\n",
      "Epoch 18/25\n",
      "336/336 [==============================] - ETA: 0s - loss: 0.4441 - accuracy: 0.8524\n",
      "Epoch 18: saving model to ./out/VGG_b3k112k224k344k460u185u2100p1_0.16p2_0.21.h5\n",
      "336/336 [==============================] - 7s 21ms/step - loss: 0.4441 - accuracy: 0.8524 - val_loss: 1.0227 - val_accuracy: 0.6536\n",
      "Epoch 19/25\n",
      "335/336 [============================>.] - ETA: 0s - loss: 0.4068 - accuracy: 0.8684\n",
      "Epoch 19: saving model to ./out/VGG_b3k112k224k344k460u185u2100p1_0.16p2_0.21.h5\n",
      "336/336 [==============================] - 7s 21ms/step - loss: 0.4063 - accuracy: 0.8686 - val_loss: 1.0493 - val_accuracy: 0.6615\n",
      "Epoch 20/25\n",
      "336/336 [==============================] - ETA: 0s - loss: 0.3631 - accuracy: 0.8855\n",
      "Epoch 20: saving model to ./out/VGG_b3k112k224k344k460u185u2100p1_0.16p2_0.21.h5\n",
      "336/336 [==============================] - 7s 21ms/step - loss: 0.3631 - accuracy: 0.8855 - val_loss: 1.0689 - val_accuracy: 0.6584\n",
      "Epoch 21/25\n",
      "334/336 [============================>.] - ETA: 0s - loss: 0.3319 - accuracy: 0.8971\n",
      "Epoch 21: saving model to ./out/VGG_b3k112k224k344k460u185u2100p1_0.16p2_0.21.h5\n",
      "336/336 [==============================] - 7s 21ms/step - loss: 0.3312 - accuracy: 0.8975 - val_loss: 1.0531 - val_accuracy: 0.6671\n",
      "Epoch 22/25\n",
      "334/336 [============================>.] - ETA: 0s - loss: 0.2958 - accuracy: 0.9118\n",
      "Epoch 22: saving model to ./out/VGG_b3k112k224k344k460u185u2100p1_0.16p2_0.21.h5\n",
      "336/336 [==============================] - 7s 21ms/step - loss: 0.2951 - accuracy: 0.9121 - val_loss: 1.0826 - val_accuracy: 0.6632\n",
      "Epoch 23/25\n",
      "336/336 [==============================] - ETA: 0s - loss: 0.2653 - accuracy: 0.9211\n",
      "Epoch 23: saving model to ./out/VGG_b3k112k224k344k460u185u2100p1_0.16p2_0.21.h5\n",
      "336/336 [==============================] - 7s 22ms/step - loss: 0.2653 - accuracy: 0.9211 - val_loss: 1.0751 - val_accuracy: 0.6762\n",
      "Epoch 24/25\n",
      "335/336 [============================>.] - ETA: 0s - loss: 0.2324 - accuracy: 0.9349\n",
      "Epoch 24: saving model to ./out/VGG_b3k112k224k344k460u185u2100p1_0.16p2_0.21.h5\n",
      "336/336 [==============================] - 7s 22ms/step - loss: 0.2321 - accuracy: 0.9350 - val_loss: 1.1967 - val_accuracy: 0.6419\n",
      "Epoch 25/25\n",
      "335/336 [============================>.] - ETA: 0s - loss: 0.2049 - accuracy: 0.9442\n",
      "Epoch 25: saving model to ./out/VGG_b3k112k224k344k460u185u2100p1_0.16p2_0.21.h5\n",
      "336/336 [==============================] - 8s 23ms/step - loss: 0.2046 - accuracy: 0.9443 - val_loss: 1.1552 - val_accuracy: 0.6645\n",
      "12/48 [======>.......................] - ETA: 0s - loss: 1.3211 - accuracy: 0.6432"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-06 23:21:13.619811: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 1s 11ms/step - loss: 1.3522 - accuracy: 0.6343\n",
      "loss ---> 1.3522313833236694\n",
      "accuracy ---> 0.6342894434928894\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.005),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "## if os.path.exists(f'./out/{model_name}.h5'): os.system(f\"rm ./out/{model_name}.h5\")\n",
    "## if os.path.exists(f'./out/{model_name}_INI.h5'): os.system(f\"rm ./out/{model_name}_INI.h5\")\n",
    "## if os.path.exists(f'./out/{model_name}_parsed.h5'): os.system(f\"rm ./out/{model_name}_parsed.h5\")\n",
    "\n",
    "# add checkpoint callback\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "                f'./out/{model_name}.h5',\n",
    "                monitor='val_accuracy',\n",
    "                verbose=2,\n",
    "                save_best=True,\n",
    "                save_weights_only=False,\n",
    "                mode='max')\n",
    "\n",
    "# add early stopping callback\n",
    "print(\"\\nConfiguring early stopping parameters.\")\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                min_delta=.005,\n",
    "                patience=5,\n",
    "                verbose=2,\n",
    "                mode='auto')\n",
    "\n",
    "hist = model.fit(\n",
    "                train_ds,\n",
    "                validation_data=val_ds,\n",
    "                epochs=25,\n",
    "                verbose=1,\n",
    "                callbacks=[checkpoint])\n",
    "\n",
    "#loss, acc = model.evaluate(x=x_test, y=y_test, verbose=1)\n",
    "m = tf.keras.models.load_model(f'./out/{model_name}.h5')\n",
    "loss, acc = m.evaluate(x_test, y_test, verbose=1)\n",
    "print(f\"loss ---> {loss}\\naccuracy ---> {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create config file for snntoolbox\n",
    "config = configparser.ConfigParser()\n",
    "\n",
    "# set up data/output paths\n",
    "config['paths'] = {\n",
    "    'path_wd': './out',\n",
    "    'dataset_path': './data',\n",
    "    'runlabel': model_name,\n",
    "    'filename_ann': model_name\n",
    "}\n",
    "\n",
    "# configure tools\n",
    "config['tools'] = {\n",
    "    'evaluate_ann': False,\n",
    "    'parse': True,\n",
    "    'normalize': True,\n",
    "    'simulate': True\n",
    "}\n",
    "\n",
    "# configure conversion parameters\n",
    "config['conversion'] = {\n",
    "    'max2avg_pool': True\n",
    "}\n",
    "\n",
    "# configure simulation settings\n",
    "config['simulation'] = {\n",
    "    'simulator': 'INI',\n",
    "    'duration': 64,\n",
    "    'batch_size': 24,\n",
    "    'num_to_test': 120,\n",
    "    'keras_backend': 'tensorflow'\n",
    "}\n",
    "\n",
    "# configure the cell parameters\n",
    "config['cell'] = {\n",
    "    'reset': \"\"\"'Reset by subtraction'\"\"\"\n",
    "}\n",
    "\n",
    "# configure output parameters\n",
    "config['output'] = {\n",
    "    'plot_vars': {\n",
    "        # 'input_image',\n",
    "        # 'spiketrains',\n",
    "        # 'spikerates',\n",
    "        # 'spikecounts',\n",
    "        # 'operations',\n",
    "        # 'normalization_activations',\n",
    "        # 'activations',\n",
    "        # 'correlation',\n",
    "        # 'v_mem',\n",
    "        # 'error_t'\n",
    "    },\n",
    "    'verbose': 0,\n",
    "    'overwrite': True\n",
    "}\n",
    "\n",
    "with open('./config.ini', 'w') as configfile:\n",
    "    config.write(configfile)\n",
    "\n",
    "# run snn conversion/simulation\n",
    "#snn.main(\"./config.ini\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "56c951395e02373f5502f328cefe5acd11c56a78011cd187363e8cc6471715bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
